{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №4: обработка текстов, нейронные сети на keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 14pt\">Дедлайн: 13 сентября 23:59</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оформление дз**: \n",
    "\n",
    "- Task short name: ``NN_NLP``.\n",
    "- Выполненное дз сохраните в файл ``ML2018_<фамилия>_HW#.ipynb``, к примеру -- ``ML2018_ivanov_HW4.ipynb``\n",
    "- Присылайте выполненное задание на почту `` mailto:ml4megafon_2018_08@bigdatateam.org `` с темой письма `` HW# Short name. ФИО ``. \n",
    "\n",
    "    Например: `` HW4 NN_NLP. Иванов Иван Иванович. ``\n",
    "\n",
    "**Вопросы**:\n",
    "- Свои вопросы присылайте в Telegram.\n",
    "\n",
    "**Фидбек**:\n",
    "- Пожалуйста, оставьте свой отзыв после выполнения домашнего задания по сссылке:\n",
    "\n",
    "    http://bit.ly/ml4megafon_august18_hw4keras_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/ml/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/data/anaconda3/envs/ml/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 0. Векторизация текстов, повторение.\n",
    "\n",
    "#### Bag of Words (CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала воспользуемся подходом мешка слов (`bag of words`). Он создает вектор длиной в количество уникальных слов во всех текстах, подсчитывает количество вхождений каждого слова в каждый текст и подставляет это число на соответствующую позицию в векторе. Данный метод доступен в модуле `sklearn.feature_extraction.text` в классе `CountVectorizer`. Но столь простой метод можно реализовать и самостоятельно. Для Вашего удобства код, реализующий данный функционал, приведен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "[1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "[1. 1. 1. 0. 2. 1. 1. 2.]\n",
      "[0. 0. 0. 1. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "texts = [['i', 'have', 'a', 'cat'],\n",
    "         ['he', 'have', 'a', 'dog'], # Не обращайте внимания на грамматику, считаем слова приведенными в начальную форму\n",
    "         ['he', 'and', 'i', 'have', 'a', 'cat', 'and', 'a', 'dog'],\n",
    "         ['i', 'have', 'a', 'pencil']\n",
    "        ]\n",
    "\n",
    "dictionary = list(enumerate(set(reduce(lambda x, y: x + y, texts))))\n",
    "\n",
    "def vectorize(text):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for i, word in dictionary:\n",
    "        num = 0\n",
    "        for w in text:\n",
    "            if w == word:\n",
    "                num += 1\n",
    "        if num:\n",
    "            vector[i] = num\n",
    "    return vector\n",
    "\n",
    "for t in texts:\n",
    "    print(vectorize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://habrastorage.org/files/549/810/b75/549810b757f94e4784b6780d84a1112a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "vectorized = count_vectorizer.fit_transform([' '.join(x) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'cat', 'dog', 'have', 'he', 'pencil']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Имена признаков (т.е. слов, которые употребляются в тексте)\n",
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0],\n",
       "       [2, 1, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем Ваше внимание, что размерность данных векторов стала меньше. Это произошло из-за стандартных настроек `CountVectorizer`'а: по умолчанию рассматриваются только токены длины 2 и выше. Ниже приведена выдержка из документации.\n",
    "```\n",
    "token_pattern : string\n",
    "\n",
    "Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из основных классических механизмов является TF-IDF [вики](https://ru.wikipedia.org/wiki/TF-IDF), который позволяет получать достаточно информативное представление текстов. Его так же можно импортировать из `sklearn.feature_extraction.text `. Ниже приведен пример его вызова и преобразования тех же игрушечных текстов, что использовались выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_texts = vectorizer.fit_transform([' '.join(x) for x in texts])\n",
    "# Не забывайте, что у vectorizer есть и метод fit_transform, и метод transform\n",
    "vectorized_texts_without_fitting = vectorizer.transform([' '.join(x) for x in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1. Линейные модели (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать тексты новостей из датасета `` The 20 newsgroups text dataset ``. По тексту новости требуется определить наиболее вероятную категорию (иначе говоря, тему). Для начала будем работать только с двумя категориями. Обучите линейный классификатор для разделения двух классов. Данные разделены на train и test с помощью `train_test_split`. Обратите внимание на исходный формат: тексты доступны в поле `.data`, метки классов в поле `.target`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала рассмотрим категории `easy_categories`, которые сильно разнятся \"на глаз\": огнестрельное оружие и научные заметки о космосе.\n",
    "(Затем проделайте все те же действия для другой пары тематик `hard_categories`: автомобильные новости о автомобилях и мотоциклах соответственно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_categories = ['talk.politics.guns', 'sci.space']\n",
    "hard_categories = ['rec.motorcycles', 'rec.autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(c):\n",
    "    return fetch_20newsgroups(subset='all', \n",
    "                                     categories=c,\n",
    "                                     remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_LogisticRegression(train_texts, test_texts, train_targets, test_targets, vectorizer):\n",
    "    data_train = vectorizer.fit_transform(train_texts)\n",
    "    # Не забывайте, что у vectorizer есть и метод fit_transform, и метод transform\n",
    "    data_test = vectorizer.transform(test_texts)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(data_train, train_targets)\n",
    "    data_test_pred = lr.predict(data_test)\n",
    "    data_test_f1_s = f1_score(test_targets, data_test_pred, average='macro')\n",
    "    test_targets_accuracy_score = accuracy_score(test_targets, data_test_pred)\n",
    "    return data_test_f1_s, test_targets_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_data_easy = get_groups(easy_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_easy, test_texts_easy, train_targets_easy, test_targets_easy = train_test_split(two_groups_data_easy.data, \n",
    "                                                                        two_groups_data_easy.target, \n",
    "                                                                        test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_data_hard = get_groups(hard_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_hard, test_texts_hard, train_targets_hard, test_targets_hard = train_test_split(two_groups_data_hard.data, \n",
    "                                                                        two_groups_data_hard.target, \n",
    "                                                                        test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем тексты в разреженные векторы с помощью `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()# TfidfVectorizer() \n",
    "data_train = vectorizer.fit_transform(train_texts)\n",
    "# Не забывайте, что у vectorizer есть и метод fit_transform, и метод transform\n",
    "data_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите логистическую регрессию. Оцените качество классификации на отложенной выборке с помощью `accuracy` и `f1_score` ([Wikipedia](https://en.wikipedia.org/wiki/F1_score))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните качество классификации (по метрике `accuracy`) на первой и второй паре тематик (`easy_categories` и `hard_categories` соответственно)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проделайте аналогичные манипуляции (векторизация текстов -> обучение модели -> предсказание на тестовой выборке -> оценка результатов с помощью метрики `accuracy_score`) используя `TfidfVectorizer` для векторизации. Стали ли результаты лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_l_f1_score, easy_l_accuracy_score = calc_LogisticRegression(train_texts_easy, test_texts_easy, train_targets_easy, test_targets_easy, CountVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_l_f1_score, hard_l_accuracy_score = calc_LogisticRegression(train_texts_hard, test_texts_hard, train_targets_hard, test_targets_hard, CountVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9041010686026594\n"
     ]
    }
   ],
   "source": [
    "print(easy_l_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9043062200956937\n"
     ]
    }
   ],
   "source": [
    "print(easy_l_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8161692587922096\n"
     ]
    }
   ],
   "source": [
    "print(hard_l_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8170731707317073\n"
     ]
    }
   ],
   "source": [
    "print(hard_l_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy_l_f1_score < easy_l_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard_l_f1_score < hard_l_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy > hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_t_f1_score, easy_t_accuracy_score = calc_LogisticRegression(train_texts_easy, test_texts_easy, train_targets_easy, test_targets_easy, TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_t_f1_score, hard_t_accuracy_score = calc_LogisticRegression(train_texts_hard, test_texts_hard, train_targets_hard, test_targets_hard, TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9328282828282829\n"
     ]
    }
   ],
   "source": [
    "print(easy_t_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9330143540669856\n"
     ]
    }
   ],
   "source": [
    "print(easy_t_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829268292682927\n"
     ]
    }
   ],
   "source": [
    "print(hard_t_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8292682926829268\n"
     ]
    }
   ],
   "source": [
    "print(hard_t_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy_t_f1_score < easy_t_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard_t_f1_score = hard_t_accuracy_score !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy > hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2. Random Forest (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем работать со всеми 20 категориями до определенной даты (`subset='train'`). Разделите выборку на обучающую и валидационную в отношении 90 на 10 (не забудьте перемешать данные). Переведите тексты в векторное представление с помощью `TfidfVectorizer`. Попробуйте обучить `Random Forest` для решения данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = fetch_20newsgroups(subset='train', \n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "texts = whole_data.data\n",
    "labels = whole_data.target\n",
    "labels_index = {name: idx \n",
    "                for idx, name in enumerate(whole_data.target_names)}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать для векторизации `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_texts = vectorizer.fit_transform(texts)# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_t, test_texts_t, train_targets_t, test_targets_t = train_test_split(texts,labels,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_RandomForestClassifier(train_texts, test_texts, train_targets, test_targets, vectorizer, lr):\n",
    "    data_train = vectorizer.fit_transform(train_texts)\n",
    "    # Не забывайте, что у vectorizer есть и метод fit_transform, и метод transform\n",
    "    data_test = vectorizer.transform(test_texts)\n",
    "    lr.fit(data_train, train_targets)\n",
    "    data_test_pred = lr.predict(data_test)\n",
    "    data_test_f1_s = f1_score(test_targets, data_test_pred, average='macro')\n",
    "    test_targets_accuracy_score = accuracy_score(test_targets, data_test_pred)\n",
    "    return data_test_f1_s, test_targets_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_t_f1_score, texts_t_accuracy_score = calc_RandomForestClassifier(train_texts_t, test_texts_t, train_targets_t, test_targets_t, TfidfVectorizer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434790932854658\n"
     ]
    }
   ],
   "source": [
    "print(texts_t_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4507230851633637\n"
     ]
    }
   ],
   "source": [
    "print(texts_t_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите, как менялись значения критерия качества `accuracy` на валидационной выборке при увеличении параметра `n_estimators`. Видны ли следы переобучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ts = []\n",
    "for i in [5, 10, 20, 100]:\n",
    "    texts_t_f1_score_1, texts_t_accuracy_score_1 = calc_RandomForestClassifier(train_texts_t, test_texts_t, train_targets_t, test_targets_t, TfidfVectorizer(), RandomForestClassifier(n_estimators=i))\n",
    "    texts_ts.append( (texts_t_f1_score_1, texts_t_accuracy_score_1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3575898181390032, 0.36207820032137117), (0.4515149481825687, 0.4692019282271023), (0.49392698959622805, 0.516336368505624), (0.6155091762266174, 0.6347080878414569)]\n"
     ]
    }
   ],
   "source": [
    "print(texts_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# С увеличением n_estimators accuracy увеличивается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3. Embeddings & Neural Networks (55%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся к нейронным сетям для решения данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь воспользуемся предобученным представлением для слов GloVe (подробнее [здесь](https://nlp.stanford.edu/projects/glove/)). В переменной `GLOVE_DIR` укажите директорию, в которой находится предобученное представление GloVe.\n",
    "\n",
    "Будем работать с последовательностями не длиннее `MAX_SEQUENCE_LENGTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 400000 представлений слов.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = ''\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Загружено %s представлений слов.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся классом `Tokenizer` из `keras.preprocessing.text` для первоначального кодирования слов. Приведем строки к единой длинне используя паддинг с помощью `pad_sequences` из `keras.preprocessing.sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные содержат 105372 уникальных токенов.\n",
      "Shape of data tensor: (11314, 1000)\n",
      "Shape of label tensor: (11314, 20)\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Данные содержат %s уникальных токенов.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на обучающую (`x_train, y_train`) и валидационную (`x_val, y_val`) в соотношении 90 на 10. Валидационная выборка понадобится для оценки качества классификации в процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(data,labels,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь воспользуемся GloVe для кодирования слов. Для этого сформируем `embedding_matrix` и подадим ее в качестве инициализационной константы `Embedding` слою из `keras.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = keras.layers.Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведен простой вариант сети. Улучшите ее и получите хотя бы 60% Accuracy на валидационной выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 1D convnet with global maxpooling\n",
    "sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# Your architecture here\n",
    "x = keras.layers.GlobalMaxPooling1D()(embedded_sequences)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "preds = keras.layers.Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/10\n",
      "10182/10182 [==============================] - 2s 205us/step - loss: 2.9412 - acc: 0.0973 - val_loss: 2.8636 - val_acc: 0.1201\n",
      "Epoch 2/10\n",
      "10182/10182 [==============================] - 1s 89us/step - loss: 2.7948 - acc: 0.1620 - val_loss: 2.7480 - val_acc: 0.1758\n",
      "Epoch 3/10\n",
      "10182/10182 [==============================] - 1s 118us/step - loss: 2.6780 - acc: 0.2034 - val_loss: 2.6539 - val_acc: 0.2226\n",
      "Epoch 4/10\n",
      "10182/10182 [==============================] - 1s 96us/step - loss: 2.5839 - acc: 0.2370 - val_loss: 2.5697 - val_acc: 0.2297\n",
      "Epoch 5/10\n",
      "10182/10182 [==============================] - 1s 124us/step - loss: 2.5037 - acc: 0.2615 - val_loss: 2.5026 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      "10182/10182 [==============================] - 1s 141us/step - loss: 2.4412 - acc: 0.2779 - val_loss: 2.4530 - val_acc: 0.2597\n",
      "Epoch 7/10\n",
      "10182/10182 [==============================] - 1s 93us/step - loss: 2.3909 - acc: 0.2868 - val_loss: 2.4131 - val_acc: 0.2889\n",
      "Epoch 8/10\n",
      "10182/10182 [==============================] - 1s 102us/step - loss: 2.3447 - acc: 0.3037 - val_loss: 2.3608 - val_acc: 0.3065\n",
      "Epoch 9/10\n",
      "10182/10182 [==============================] - 1s 82us/step - loss: 2.3015 - acc: 0.3172 - val_loss: 2.3248 - val_acc: 0.3207\n",
      "Epoch 10/10\n",
      "10182/10182 [==============================] - 1s 123us/step - loss: 2.2680 - acc: 0.3242 - val_loss: 2.2945 - val_acc: 0.3330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a828f0588>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Для улучшения результатов можно добавить слоев (например `Conv1D` и `MaxPooling1D`), поменять оптимизатор или сделать что-то еще :)\n",
    "\n",
    "В ходе обучения модель может начать расходиться (иногда так случается), так что рекомендуем сохранять версию сети, показавшую лучший результат на данный момент (это можно сделать с помощью `model.save_weights`). Загрузить модель можно с помощью `model.load_weights`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4. Проверка модели в *реальных* условиях (10%)\n",
    "\n",
    "В реальной жизни новые данные становятся доступны с течением времени. Проверьте качество лучшей нейросети и Random Forest'а на отложенной выборке. В ней содержатся данные, полученные после определенной даты. На обучающей выборке использовались лишь данные, доступные до этой даты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_test_data = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "delayed_texts = whole_data.data\n",
    "delayed_labels = whole_data.target\n",
    "delayed_labels_index = {name: idx \n",
    "                for idx, name in enumerate(delayed_test_data.target_names)}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем Ваше внимание, приводить текстовую информацию в векторный вид нужно тем же `transformer`'ом, что использовался при обучении/валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пожалуйста, оставьте отзыв о домашнем задании: [link](http://bit.ly/ml4megafon_august18_hw4keras_feedback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
