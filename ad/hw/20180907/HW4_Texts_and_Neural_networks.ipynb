{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №4: обработка текстов, нейронные сети на keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 14pt\">Дедлайн: 13 сентября 23:59</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оформление дз**: \n",
    "\n",
    "- Task short name: ``NN_NLP``.\n",
    "- Выполненное дз сохраните в файл ``ML2018_<фамилия>_HW#.ipynb``, к примеру -- ``ML2018_ivanov_HW4.ipynb``\n",
    "- Присылайте выполненное задание на почту `` mailto:ml4megafon_2018_08@bigdatateam.org `` с темой письма `` HW# Short name. ФИО ``. \n",
    "\n",
    "    Например: `` HW4 NN_NLP. Иванов Иван Иванович. ``\n",
    "\n",
    "**Вопросы**:\n",
    "- Свои вопросы присылайте в Telegram.\n",
    "\n",
    "**Фидбек**:\n",
    "- Пожалуйста, оставьте свой отзыв после выполнения домашнего задания по сссылке:\n",
    "\n",
    "    http://bit.ly/ml4megafon_august18_hw4keras_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 0. Векторизация текстов, повторение.\n",
    "\n",
    "#### Bag of Words (CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала воспользуемся подходом мешка слов (`bag of words`). Он создает вектор длиной в количество уникальных слов во всех текстах, подсчитывает количество вхождений каждого слова в каждый текст и подставляет это число на соответствующую позицию в векторе. Данный метод доступен в модуле `sklearn.feature_extraction.text` в классе `CountVectorizer`. Но столь простой метод можно реализовать и самостоятельно. Для Вашего удобства код, реализующий данный функционал, приведен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [['i', 'have', 'a', 'cat'],\n",
    "         ['he', 'have', 'a', 'dog'], # Не обращайте внимания на грамматику, считаем слова приведенными в начальную форму\n",
    "         ['he', 'and', 'i', 'have', 'a', 'cat', 'and', 'a', 'dog'],\n",
    "         ['i', 'have', 'a', 'pencil']\n",
    "        ]\n",
    "\n",
    "dictionary = list(enumerate(set(reduce(lambda x, y: x + y, texts))))\n",
    "\n",
    "def vectorize(text):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for i, word in dictionary:\n",
    "        num = 0\n",
    "        for w in text:\n",
    "            if w == word:\n",
    "                num += 1\n",
    "        if num:\n",
    "            vector[i] = num\n",
    "    return vector\n",
    "\n",
    "for t in texts:\n",
    "    print(vectorize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://habrastorage.org/files/549/810/b75/549810b757f94e4784b6780d84a1112a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "vectorized = count_vectorizer.fit_transform([' '.join(x) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Имена признаков (т.е. слов, которые употребляются в тексте)\n",
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем Ваше внимание, что размерность данных векторов стала меньше. Это произошло из-за стандартных настроек `CountVectorizer`'а: по умолчанию рассматриваются только токены длины 2 и выше. Ниже приведена выдержка из документации.\n",
    "```\n",
    "token_pattern : string\n",
    "\n",
    "Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из основных классических механизмов является TF-IDF [вики](https://ru.wikipedia.org/wiki/TF-IDF), который позволяет получать достаточно информативное представление текстов. Его так же можно импортировать из `sklearn.feature_extraction.text `. Ниже приведен пример его вызова и преобразования тех же игрушечных текстов, что использовались выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_texts = vectorizer.fit_transform([' '.join(x) for x in texts])\n",
    "# Не забывайте, что у vectorizer есть и метод fit_transform, и метод transform\n",
    "vectorized_texts_without_fitting = vectorizer.transform([' '.join(x) for x in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1. Линейные модели (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать тексты новостей из датасета `` The 20 newsgroups text dataset ``. По тексту новости требуется определить наиболее вероятную категорию (иначе говоря, тему). Для начала будем работать только с двумя категориями. Обучите линейный классификатор для разделения двух классов. Данные разделены на train и test с помощью `train_test_split`. Обратите внимание на исходный формат: тексты доступны в поле `.data`, метки классов в поле `.target`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала рассмотрим категории `easy_categories`, которые сильно разнятся \"на глаз\": огнестрельное оружие и научные заметки о космосе.\n",
    "(Затем проделайте все те же действия для другой пары тематик `hard_categories`: автомобильные новости о автомобилях и мотоциклах соответственно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_categories = ['talk.politics.guns', 'sci.space']\n",
    "hard_categories = ['rec.motorcycles', 'rec.autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_data = fetch_20newsgroups(subset='all', \n",
    "                                     categories=easy_categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_targets, test_targets = train_test_split(two_groups_data.data, \n",
    "                                                                        two_groups_data.target, \n",
    "                                                                        test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем тексты в разреженные векторы с помощью `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()# TfidfVectorizer() \n",
    "data_train = vectorizer.fit_transform(train_texts)\n",
    "# Не забывайте, что у vectorizer есть и метод fit_transform, и метод transform\n",
    "data_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите логистическую регрессию. Оцените качество классификации на отложенной выборке с помощью `accuracy` и `f1_score` ([Wikipedia](https://en.wikipedia.org/wiki/F1_score))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните качество классификации (по метрике `accuracy`) на первой и второй паре тематик (`easy_categories` и `hard_categories` соответственно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проделайте аналогичные манипуляции (векторизация текстов -> обучение модели -> предсказание на тестовой выборке -> оценка результатов с помощью метрики `accuracy_score`) используя `TfidfVectorizer` для векторизации. Стали ли результаты лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2. Random Forest (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем работать со всеми 20 категориями до определенной даты (`subset='train'`). Разделите выборку на обучающую и валидационную в отношении 90 на 10 (не забудьте перемешать данные). Переведите тексты в векторное представление с помощью `TfidfVectorizer`. Попробуйте обучить `Random Forest` для решения данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = fetch_20newsgroups(subset='train', \n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "texts = whole_data.data\n",
    "labels = whole_data.target\n",
    "labels_index = {name: idx \n",
    "                for idx, name in enumerate(whole_data.target_names)}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать для векторизации `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_texts = vectorizer.fit_transform()# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for train-val split here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите, как менялись значения критерия качества `accuracy` на валидационной выборке при увеличении параметра `n_estimators`. Видны ли следы переобучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3. Embeddings & Neural Networks (55%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся к нейронным сетям для решения данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь воспользуемся предобученным представлением для слов GloVe (подробнее [здесь](https://nlp.stanford.edu/projects/glove/)). В переменной `GLOVE_DIR` укажите директорию, в которой находится предобученное представление GloVe.\n",
    "\n",
    "Будем работать с последовательностями не длиннее `MAX_SEQUENCE_LENGTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = ''\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Загружено %s представлений слов.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся классом `Tokenizer` из `keras.preprocessing.text` для первоначального кодирования слов. Приведем строки к единой длинне используя паддинг с помощью `pad_sequences` из `keras.preprocessing.sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Данные содержат %s уникальных токенов.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на обучающую (`x_train, y_train`) и валидационную (`x_val, y_val`) в соотношении 90 на 10. Валидационная выборка понадобится для оценки качества классификации в процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь воспользуемся GloVe для кодирования слов. Для этого сформируем `embedding_matrix` и подадим ее в качестве инициализационной константы `Embedding` слою из `keras.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = keras.layers.Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведен простой вариант сети. Улучшите ее и получите хотя бы 60% Accuracy на валидационной выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 1D convnet with global maxpooling\n",
    "sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# Your architecture here\n",
    "x = keras.layers.GlobalMaxPooling1D()(embedded_sequences)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "preds = keras.layers.Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Для улучшения результатов можно добавить слоев (например `Conv1D` и `MaxPooling1D`), поменять оптимизатор или сделать что-то еще :)\n",
    "\n",
    "В ходе обучения модель может начать расходиться (иногда так случается), так что рекомендуем сохранять версию сети, показавшую лучший результат на данный момент (это можно сделать с помощью `model.save_weights`). Загрузить модель можно с помощью `model.load_weights`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4. Проверка модели в *реальных* условиях (10%)\n",
    "\n",
    "В реальной жизни новые данные становятся доступны с течением времени. Проверьте качество лучшей нейросети и Random Forest'а на отложенной выборке. В ней содержатся данные, полученные после определенной даты. На обучающей выборке использовались лишь данные, доступные до этой даты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_test_data = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "delayed_texts = whole_data.data\n",
    "delayed_labels = whole_data.target\n",
    "delayed_labels_index = {name: idx \n",
    "                for idx, name in enumerate(delayed_test_data.target_names)}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем Ваше внимание, приводить текстовую информацию в векторный вид нужно тем же `transformer`'ом, что использовался при обучении/валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пожалуйста, оставьте отзыв о домашнем задании: [link](http://bit.ly/ml4megafon_august18_hw4keras_feedback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
